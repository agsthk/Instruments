# evaluate_leak.py
- Want to get an idea of the background before and after the leak fixed
	- With both possible dates that leak was introduced
	- Going to take within 2 days of before and after leak fixed to visually compare
		- Actually doing within 3 days
	- After looking at June 13 vs June 24 date, leak was definitely introduced June 13 - satisfied with that!
- Want to look at the NOx as well just to have an idea of what effect that might have
	- Only have NOx measurements from B203, but NO is basically zero in that room - presumably similar?
- Did same background comparison with Phase II data (3 days difference)
# ICARTT files
 - Going to create the ICARTT header files for the Phase I data, though waiting to put the bias info in there until I talk to Megan
 - First, created the Phase II R1 headers that I'll use to share the actually background corrected data
 - Then, created the Phase I R2 header for O3 and R1 headers for other instruments
 - Running icartt_data.py to get the R1 phase II data and updated phase I data (most of which will be overwritten after I comment on the leak)
	 - Didn't work lol
	 - Did work for vent CO2, only ran for uncalibrated data
	 - But then for vent CO2 needed to add a comment about the time offset correction - did that and re-ran
# icartt_data.py
- Need to fix directory identification for calibrated data to not look in DAQ folder for phase I
- Moving determination of campaign start and stop to be earlier to use for selection of directories
- Revised to look in DAQ directory for 2025 data, and for 2024 data look in SD if it exists and Logger if it doesn't
- Okay R0 data for Phase II does appear to be zero corrected actually?? Checking git to see why the correction was missing when I ran calibration script for phase I data
	- oh wait jk this was overwritten using the updated cal files, what I uploaded was NOT corrected
		- Double checked and that is indeed true!
- In the future, I do want to split this by campaign, but not worth it for now I think
- Thinking about the campaign headers - some need different (pre vs post leak introduced/fixed) - since changes happened in the middle of the day, revising campaign start/stop to be timestamps rather than just dates
	- Modified to take full timestamps
- Ran for all non-leak periods and added to Teams after checking ICARTT scanner
- Now it's just the leak
- Before I talk to Megan, want some pre-post leak from Phase II for formaldehyde and NOx
# evaluate_leak.py
- Basically just repeating the code for comparing the ozone, but with the formaldehyde and NOx
- After talking to Megan, going to do a couple things
	- Use the phase II decays to estimate upper bias (median of peak concentrations for 5 minutes after max reached)
		- 75% is what I'm settling on :)
	- Use phase I background to estimate lower bias
		- Afternoon and nighttime
		- Did a grouped median for every 30 minutes and found that the median bias was ~40% of the reported value
			- Also that when I changed the window size
			- When I checked the phase II bias, it's about 90% (harder to say, but that's what I'm going with)
	- For Picarro (Phase II), 70% is what we're going with
	- Not estimating for NOx because it's too variable
- Now, going to add the biases that I have determined to the ICARTT headers and create the ICARTT files :)))))))
	- Took a hot second to make sure the YAML files were what I wanted, but I think it's all good
	- Running through ICARTT checker, then will post
		- THEY ALL PASSED!!!!!!!!!!
- Uploaded ICARTT files - I'M FREEEEEEEEEE
	- Still need to do ACRs and Aranet sensor ICARTT and occupancy but overall I'M BASICALLY FREEEEEE
# combine_aranet.py
- Created script to combine the two aranet sensor measurements I have into one larger file
- Have it read in the clean Aranet data and concatenate by sensor
	- Identify unique, sort by the start time
	- Clean data has the averaging time accounted for
- Have it rename the measurement columns with the sensor ID
- Join on the start times
	- Get the appropriate corresponding stop times and local times and remove duplicate columns ("\_right")
- Export the joined file to Aranet4_CleanData/Aranet4_CleanLoggerData - wait
- Can't join on start times because they aren't the exact same :(
- Okay looking at the data and the times repeat???
	- it gives the temperature twice :/ once in C and once in F, everything else repeats
		- Only 1FB20 does that actually? and it's not always in right order
	- Also 1F16F has temperature in F for sure - need to convert it
	- What I'm going to do is sort by the temperature within a repeat time and drop the larger one (F)
	- And for the other one just going to convert it
		- They vary actually - just going to sort, drop the F if it's duplicate, then convert if temp > 45
- To get them on same times, going to round datetime
- Then join on rounded datetime
- Then split by date and export split datasets
# icartt_data.py
- Shouldn't be changing the script as far as I can tell, but need to create a header input for the Aranet data
- Realized that averaging times are NOT 1 minute - measures 1 time every second
	- Going to fix that in the combining script because I don't want to deal with it otherwise
- Finished creating ICARTT header, now going to ICARTT it
- Failed - missing values not filled
	- That's because I only had the missing values going for the continuous data - moving it outside of that if block to see if that fixes it
	- It didn't because data type is "string" I guess - changed it to fill if it isn't a datetime variable and that worked